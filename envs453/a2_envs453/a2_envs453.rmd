---
title: "Geographically Weighted Regression"
author: '201347125'
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  bookdown::html_document2:
    number_figures: true
    number_sections: false
    toc: true
    smooth_scroll: true
    toc_float: true
    css: theme/style.css
    code_folding: hide
    code_download: true
    includes:
      before_body: header.html # header img

# i prefer a master bib location
bibliography: ../../../bib/library.bib
biblio-style: ../../../bib/uol.csl
---

```{r logo, echo=F}
# may include this in the header in future
htmltools::img(
  src = "theme/logo.jpg",
  alt = "logo",
  style = "position:absolute; top:0; right:5; padding:20px;\n 
  width:220px;opacity: 0.5;"
)
```

```{r setup, include=F}
# set default chunk options
knitr::opts_chunk$set(echo = T, message = F, warning = F, cache = T, results = F) # careful with cache
```

```{r, libs, echo=F}
library(sf) # simple geometry features
library(GWmodel) # gwr models
library(tidyverse) # tidy data functions
library(tmap) # map plotting
library(broom) # tidy summaries
library(kableExtra) # format tables
library(Hmisc) # regression matrix

setwd("~/Dropbox/uni/ENVS453/A2GWmodel/")
```

```{r, employ, echo=F}
nwemploy <- read.csv("./data/Data_AGE_ECOACT_UNIT.csv", header = F, skip = 2)
nwemploy <- nwemploy[, 1:(ncol(nwemploy) - 1)] # remove blank col
colnames(nwemploy) <- c(
  "ID", "LSOA", "label", "type", "typeid", "alleconact",
  "employeePT", "employeeFT", "selfempwithPT", "selfempwithFT", "selfempnoPT",
  "selfempnoFT", "unemploy", "LTsickdis"
)

# new var total employed
nwemploy$employed <- nwemploy$employeePT + nwemploy$employeeFT +
  nwemploy$selfempwithPT + nwemploy$selfempwithFT + nwemploy$selfempnoPT +
  nwemploy$selfempnoFT

# total persons
nwemploy$total <- nwemploy$employed + nwemploy$unemploy

nwemploy$unemployPC <- (nwemploy$unemploy / nwemploy$total) * 100
nwemploy$lsdPC <- (nwemploy$LTsickdis / nwemploy$alleconact) * 100

nwemploy <- nwemploy[, c(2, 6, 17:18)] # keep only econact and pc employed
```

```{r, qual, echo=F}
qual <- read.csv("./data/quals/Data_AGE_HIQUAL_UNIT.csv", header = F, skip = 2)

qual <- qual[, c(2, 6:7)]

# sum no qual + qual for total
qual$total <- qual$V6 + qual$V7
# find percentage no qual
qual$noQualPC <- (qual$V7 / qual$total) * 100

qual <- qual[, c("V2", "noQualPC")]

colnames(qual) <- c("LSOA", "noQualPC")
```

```{r, rpp, echo=F}
rpp <- read.csv("./data/rooms/Data_NPRHH_UNIT.csv", header = F, skip = 2)

rpp <- rpp[, c(2, 6:8)]

# sum over 1 person and ove 1.5 persons per room
rpp$crowded <- rpp$V7 + rpp$V8
# percentage crowding
rpp$crowdedPC <- (rpp$crowded / rpp$V6) * 100

rpp <- rpp[, c("V2", "crowdedPC")]

colnames(rpp) <- c("LSOA", "crowdedPC")
```


```{r, join, echo=F}
# sf read in lsoa geoms
nwLSOA <- st_read("./data/NorthWestLSOA.shp",
  quiet = T
)
colnames(nwLSOA)[3] <- "LSOA"

# join all data into one frame by lsoa names
nwLSOA <- list(nwLSOA, nwemploy, rpp, qual) %>%
  reduce(left_join, by = "LSOA")

nwLSOA <- nwLSOA[, c(3:8)]
```

# Introduction

```{r, area, echo=FALSE, fig.cap="Unemployment by District in the North West of England with Fisher Jenks Breaks"}
# create bg polygon for maps
bg <- nwLSOA %>%
  st_union()
bg <- tm_shape(nwLSOA) + tm_borders(lwd = 3, col = "black")

# plot pc unemployment across north west
unemp_map <- bg + tm_shape(nwLSOA) +
  tm_fill(
    col = "unemployPC", title = "Unemployment (%)",
    n = 4, # number of bins
    style = "jenks", # fisher jenks bins
    textNA = "NA",
    colorNA = "gray"
  ) +
  tm_layout(
    frame = F,
    asp = 1 / 1,
    legend.format = list(digits = 0)
  ) +
  tm_scale_bar()

unemp_map
```

## Unemployment

Looking at unemployment in the United Kingdom with 2011 census data reveals strong regional variation (See Figure \@ref(fig:area)), with levels of unemployment higher than that of many other developed countries [@Bell2010]. This variation is most notable when considering the divide between the north and south, where when controlling for outside factors, the north has overall higher levels of deprivation [@Green1988]. Notably @Brunsdon1996 coin the term 'Spatial nonstationarity' in which a global model (typically *simple linear regression* @Dobson2008) isn't sufficient to explain the relationships between variables, when considering just the global space.

Geographically weighted regression is a technique developed that allows for an observation in the variation in regression coefficients between spatial units [@Brunsdon1998], and as such allows for an observation of the local variation in regression coefficients unlike a global model.

## Data

Data is taken from the 2011 census [@ons2019], constructed by the [Office for National Statistics](https://www.ons.gov.uk/) for the UK government. The area chosen for this study consists of the North West of England (Figure \@ref(fig:area)). Data is aggregated into Lower Super Output areas (LSOA), which are defined by the ONS for the decennial census, and are the second smallest aggregational unit. There are a total of `r nrow(nwLSOA)` within the North West of the UK.

## Variables Chosen

1. Unemployment: The percentage of individuals who are unemployed compared with the total population of persons who are able to work within the population in an LSOA.

2. Long-Term Sickness: The percentage of individuals who and not economically active due to long term sickness, compared with the total number of economically active individuals within an LSOA. Selected as unemployment, deprivation and illness are often associated [@Shouls1996].

3. Crowding: The total number of residences within an LSOA with over 1 person per room, as a percentage of the total number of residences. Used as a metric to determine a level of deprivation [@Brunsdon1998].

4. No Qualifications: The percentages of individuals who have no qualifications, as a percentage of individuals within an LSOA. Lack of formal qualifications is associated with higher levels of unemployment [@Brown1997].

```{r, cont_weight, echo=F}
# contiguity distances
LSOA_nb <- poly2nb(nwLSOA)
cont_weight <- nb2listw(LSOA_nb)
```

```{r, weights,  eval = F, echo=F}
# centroid distances of each polygon
LSOAdists <- nwLSOA %>%
  st_transform(4326) %>%
  st_centroid() %>%
  st_coordinates() %>%
  dist() %>%
  as.matrix()

# function to find inverse weighting for polygons
inv_weight <- function(expon, dists) {
  weight <- 1 / (dists^expon)
  weight[LSOAdists > 20000] <- 0
  diag(weight) <- 0
  weight <- weight / rowSums(weight)
  weight <- mat2listw(weight)
}

# define different exponents with function
distInv2 <- inv_weight(2, LSOAdists)
distInv4 <- inv_weight(4, LSOAdists)

save(distInv2, file = "./data/distInv2")
save(distInv4, file = "./data/distInv4")
```

```{r, pval, echo=F}
load(file = "./data/distInv2")
load(file = "./data/distInv4")

# function to convert significance figures into p asterisks
p_val <- function(x) {
  symnum(x, corr = FALSE, na = FALSE, cutpoints = c(
    0, 0.001, 0.01, 0.05, 1
  ), symbols = c("***", "**", "*", " "))
}
```

## Global Correlations

Due to the very large number of observations, the Central Limit Theorem states that they are very likely to be normally distributed [@Rosenblatt1955], therefore a pearson correlation was chosen to compare the correlations between each chosen variable, with the results shown as a matrix on Table \@ref(tab:moran).

All variables show a reasonably strong and highly significant correlation between each other, however, surprisingly there is a significant negative correlation between the outcome variable Unemployment and both crowding, and no qualifications. This result indicates that there are likely more factors to consider when comparing these variables globally, but insight into the local variation in these variables may give differing results. For example, perhaps rural areas, with less crowding have poorer employment [@Brunsdon1998].

```{r, moran1, results=T}
# define variables in the study
vars <- as_tibble(nwLSOA[3:6])
vars <- vars[1:4]

# find correlation matrix
res <- rcorr(as.matrix(vars), type = "pearson")

res$r <- as.character(round(res$r, 2))
res$P <- p_val(res$P) %>% na.omit()
res$r[res$r == 1] <- "" # change diags to blank

# paste p asterisks to values
resTab <- paste(res$r, res$P)

resTab <- resTab %>%
  matrix(ncol = 4) %>%
  as.data.frame()

colnames(resTab) <- c("unemployPC", "lsdPC", "crowdedPC", "noQualPC")
Variable <- colnames(resTab)
resTab <- cbind(Variable, resTab)
```


```{r, moran, results=T, echo=F}
# tell Kable to not show anything for NA values
options(knitr.kable.NA = "")
# Table 2
kable(resTab,
  digits = 2,
  caption = "Correlation matrix for variables included in Unemployment Analysis",
  linesep = ""
) %>%
  # footnote for significance
  footnote(
    general_title = "",
    general = c("*** Significant at the 0.001 level")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F
  )
```

# Q1. Global Moran's I

Moran's I was calculated for each variable in this analysis, results shown on Table \@ref(Table tab:moran). Both contiguous weighting and inverse distance based weighting were used and results compared. In this instance an exponent of both 2 and 4 were chosen for the distance based weighting, increasing the exponent increases the proportional weighting assigned to observations that are spatially closer together, and as centroids are chosen, this distance relates to the size of the LSOA polygons.

The larger exponent used emphasises the nearest neighbours of each observation, through increased weighting. Comparing the distance based weighting between the 2 exponent and 4 exponent (Table \@ref(tab:moran)) indicates that Moran's I is higher when using a larger exponent, as expected, indicating a higher spatial autocorrelation between neighbours that are closer together. Each variable in this analysis shows a significant positive spatial autocorrelation. The result indicates that each variable for an observation will usually be surrounded by variables of similar values, i.e. unemployment, illness, crowding and individuals with no qualifications are clustered spatially.

In addition, Figure \@ref(fig:moranplot) shows how the spatial lag of unemployment relates to the observations of unemployment, given the narrow cluster of points with a positive correlation shows a clear positive spatial auto correlation.

```{r, MItable1, eval = F}
# function convert output for table
moran_tab <- function(df, weight) {
  moransUnemployID <- tidy(moran.test(df$unemployPC, weight))
  moransIllnessID <- tidy(moran.test(df$lsdPC, weight))
  moransrppID <- tidy(moran.test(df$crowdedPC, weight))
  moransNoqualID <- tidy(moran.test(df$noQualPC, weight))

  tb <- rbind(moransUnemployID, moransIllnessID, moransrppID, moransNoqualID)

  tb <- tb[, c(1, 4:5)]
  tb <- cbind(Row.Names = c("Unemploy", "Illness", "Crowding", "No Quals"), tb)
  colnames(tb) <- c("Variable", "Estimate", "Statistic", "p")

  # use function for my table
  tb$p <- p_val(tb$p)

  tb$Estimate <- sprintf("%.2f", tb$Estimate)
  tb$Statistic <- sprintf("%.2f", tb$Statistic)

  tb$Estimate <- paste(as.character(tb$Estimate), tb$p)
  tb <- tb[1:3]
}
tb1 <- moran_tab(nwLSOA, cont_weight)
tb2 <- moran_tab(nwLSOA, distInv2)
tb3 <- moran_tab(nwLSOA, distInv4)

save(tb1, file = "./data/tb1.RData")
save(tb2, file = "./data/tb2.RData")
save(tb3, file = "./data/tb3.RData")
```

```{r, MItable2, results=T}
load(file = "./data/tb1.RData")
load(file = "./data/tb2.RData")
load(file = "./data/tb3.RData")

table <- cbind(
  Variable = tb1$Variable,
  Contiguous = tb1[2:3],
  Distance2 = tb2[2:3],
  Distance4 = tb3[2:3]
)

# tell Kable to not show anything for NA values
options(knitr.kable.NA = "")
# Table 2
kable(table,
  digits = 2, caption = "Morans I for Each Variable",
  col.names = c("Variable", "Estimate", "Statistic", "Estimate", "Statistic", "Estimate", "Statistic"),
  linesep = ""
) %>%
  # footnote for significance
  footnote(
    general_title = "",
    general = c(
      "*** Significant at the 0.001 level"
    )
  ) %>%
  add_header_above(c(" ", "Contiguous" = 2, "Distance 2" = 2, "Distance 4" = 2)) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F
  )
```
<center>
```{r, moranplot, fig.cap="Moran's I plot for Unemployment against the spatial lag of Unemployment"}
moran.plot(nwLSOA$unemployPC, cont_weight, xlab = "Unemployment (%)", ylab = "Spatial Lag")
```
</center>

# Q2. Local Spatial Autocorrelation

While the global Moran's I gives insight into the overall spatial autocorrelation of each variable, it doesn't explore how spatial autocorrelation may vary between the units of aggregation in this study. Figure \@ref(fig:locali) highlights the variation in Moran's I for Unemployment within each LSOA, and shows how the calculation for the spatial weighting affects the results. Each map typically has similar distributions, so it could be said that altering the weighting calculations hasn't affected the local spatial autocorrelation results by much. The maps highlight the areas of significantly High Moran's I values, surrounded by High values, Low by Low, and outliers; both Low/High and High/Low. Insignificant calculations of Moran's I are excluded. Given each weighting provides a significant value for Moran's I (Table \@ref(tab:moran)), contiguous weighting is chosen for the next analysis due to its speed.

This map reveals that the far northern end of the study area, in addition to the far southern end show a number of LSOA's with high values surrounded by other high values. There is a band of very small LSOAs at the southern end with Low values of Moran's I surrounded by Low values. In some cases these low values may be negative, indicating a negative local spatial autocorrelation. 

```{r, weighting}
# list all weighting schemes
weights <- list(cont_weight, distInv2, distInv4)

# find local morans i for each weighting
lmi <- lapply(weights, function(x) {
  x <- as.data.frame(localmoran(nwLSOA$unemployPC, x))
})

# merge all with nwLSOA
loci <- lapply(lmi, function(x) {
  x <- merge(nwLSOA, x, by = 0)
})
```

```{r, localiPRE}
# find variation from the mean unemp
unempMean <- nwLSOA$unemployPC - mean(nwLSOA$unemployPC)
signif <- 0.1

# quadrant colours
colors <- c(
  rgb(0.9, 0.9, 0.9, alpha = 1), "blue",
  rgb(0.7, 0.7, 1, alpha = 1),
  rgb(1, 0.7, 0.7, alpha = 1),
  "red"
)
# blank vector
quadrant <- vector(mode = "numeric", length = nrow(nwLSOA))

# define variables for the loop
n <- 1
cnames <- c("cont", "inv2", "inv4")
quads <- data.frame(nwLSOA[1])

## loop to create quadrants for each weighting scheme
for (df in loci) {
  C_mI <- df$Ii - mean(df$Ii)

  quadrant[unempMean > 0 & C_mI > 0] <- "4. High/High"
  quadrant[unempMean < 0 & C_mI < 0] <- "1. Low/Low"
  quadrant[unempMean < 0 & C_mI > 0] <- "2. Low/High"
  quadrant[unempMean > 0 & C_mI < 0] <- "3. High/Low"

  p <- as.data.frame(lmi[[n]][, 5])

  quadrant[p > signif] <- "0. Insig."

  quads <- as_tibble(cbind(quads, quadrant))

  colnames(quads)[which(names(quads) == "quadrant")] <- cnames[n]

  n <- n + 1
}

quads <- quads[, c(1, 3:5)]

nwLSOA <- list(nwLSOA, quads) %>%
  reduce(left_join, by = "LSOA")

map <- function(col) {
  bg + tm_shape(nwLSOA) +
    tm_fill(col = col, palette = colors, style = "cat") +
    tm_layout(
      frame = F,
      asp = .5 / 1,
      legend.show = F
    )
}

cnames <- list("cont", "inv2", "inv4")
maps <- lapply(cnames, map)
```

<center>

```{r, fig.height=1, fig.width=5, echo=F}
legend <- tm_shape(nwLSOA) +
  tm_fill(
    title = "Unemployment Local I", col = "cont", palette = colors, style = "cat",
    legend.is.portrait = F
  ) +
  tm_layout(
    legend.only = T,
    legend.position = c("center", "top"),
    legend.stack = "horizontal"
  )
legend
```

```{r locali, fig.cap="Local I Cluster Maps for Unemployment showing: Contiguous Weighting (left), Inverse Distance with 2 exponent (middle), Inverse Distance with 4 exponent (right).", eval=T, echo=F}
tmap_arrange(maps)
```

</center>

```{r, localiVAR1, eval=T}
signif <- 0.1

quadrant <- vector(mode = "numeric", length = nrow(nwLSOA))
n <- 1
cnames <- c("lsdPC", "crowdedPC", "noQualPC")
quads <- data.frame(nwLSOA[1])

for (var in cnames) {
  C_mI <- loci[[1]]$Ii - mean(loci[[1]]$Ii)

  curVar <- nwLSOA[[var]]

  varMean <- curVar - mean(curVar)

  quadrant[varMean > 0 & C_mI > 0] <- "4. High/High"
  quadrant[varMean < 0 & C_mI < 0] <- "1. Low/Low"
  quadrant[varMean < 0 & C_mI > 0] <- "2. Low/High"
  quadrant[varMean > 0 & C_mI < 0] <- "3. High/Low"

  p <- as.data.frame(lmi[[1]][, 5])

  quadrant[p > signif] <- "0. Insig."

  quads <- as_tibble(cbind(quads, quadrant))

  colnames(quads)[which(names(quads) == "quadrant")] <- paste0(cnames[n], "MI")

  n <- n + 1
}
quads <- quads[, c(1, 3:5)]

nwLSOA <- list(nwLSOA, quads) %>%
  reduce(left_join, by = "LSOA")

map <- function(col) {
  bg + tm_shape(nwLSOA) +
    tm_fill(col = col, palette = colors, style = "cat") +
    tm_layout(
      frame = F,
      asp = .5 / 1,
      legend.show = F
    )
}
```

<center>

```{r, localiVAR2, fig.cap="Local I Cluster Maps with Contiguous Weighting for Other Variables: Long-Term Illness (left), Crowded (middle),  No Qualifications (right)."}
cnames <- list("lsdPCMI", "crowdedPCMI", "noQualPCMI")
maps <- lapply(cnames, map)
tmap_arrange(maps)
```

</center>

Figure \@ref(fig:localiVAR2) shows the individual variation in Local Moran's I for other variables, using contiguous weighting, these figures largely reflect the correlation between the outcome variable and predictor variables, where illness shows a positive correlation, but crowded and no qualifications show negative. However, there are clear isolated local incidences where Moran's I gives a positive result, for both.

# Q3. Geographically Weighted Regression

While a typical global regression model takes the form:

$$y_{i}=a_{0}+\sum_{k} a_{k} x_{i k}+\varepsilon_{i}$$

GWR extends the regression to allow for local parameters to be estimated:

$$y_{i}=a_{0}\left(u_{i}, v_{i}\right)+\sum_{k} a_{k}\left(u_{i}, v_{i}\right) x_{i k}+\varepsilon_{i}$$

where $\left(u_{i}, v_{i}\right)$ are the coordinates of a point in space [@Fotheringham1998].

## Results of Global Regression

The results of the global ordinary least squares regression indicate that above all, the percentage of individuals who have a long term illness within an LSOA has the largest impact on the percentage of individuals who are unemployed, a one percent increase in unemployment is associated with a 0.45% increase in long term illness (Table \@ref(tab:global2)). Surprisingly, as with the correlations between the other variables, there seems to be a slight, but significant, negative association. As both crowding and no qualifications give slight negative model estimates. This result suggests additional variables have to be considered, one of which may be the spatial association which can be explored with a Geographically Weighted Regression.

## Bandwidth Selection

When computing the Geographically Weighted Regression (GWR) model, the bandwidth must be selected to calibrate the model. See table \@ref(tab:kernels) for an overview of potential kernels.

<caption><font color="#838383">(#tab:kernels) Kernel Functions; $w_{i j}$ is the $j-th$ element of the diagonal of the matrix of geographical weights $W (u_i,v_i)$, and $d_ij$ is the distance between observations $i$ and $j$, and $b$ is the bandwidth (@Charlton2015). </font>

Kernels       | Equation
------------- | -------------
Global Model  | $w_{i j}=1$
Gaussian      | $w_{i j}=\exp \left(-\frac{1}{2}\left(\frac{d_{i j}}{b}\right)^{2}\right)$
Bi-square     | $w_{i j}=\left\{\begin{array}{cc}{\left(1-\left(d_{i j} / b\right)^{2}\right)^{2}} & {\text { if }\left|d_{i j}\right|<b,} \\ {0} & {\text { otherwise }}\end{array}\right.$

Bandwidth $b$ is the key parameter for all kernel functions, for this purpose, given the large variation in the size of LSOA's, a discontinuous kernel function, Bi-square, was chosen (See Table \@ref(tab:kernels)). The bi-square kernel works similarly to the Gaussian kernel, but gives null weighting to observations with a distance greater than $b$ [@Charlton2015]. While it is possible to provide a bandwidth with a varying distance, but a fixed number of data, this method gave a very small number of observations that wouldn't allow for computation, and typically fixed bandwidth is suited to regular sample configuration such as census data [@Charlton2015]. The size of a kernel is indicated by its bandwidth, meaning a small bandwidth provides close neighbours with larger weighting, while more distant neighbours are given smaller weights. To select the bandwidth for the GWR model, the $R$ [@DevelopmentCoreTeam2019] package `GWmodel` was used with the function `gwr.basic` [@Lu2014a].

```{r, global, echo=F, results=T}
lm_model <- lm(unemployPC ~ lsdPC + crowdedPC + noQualPC, data = nwLSOA)

## Prepare data for Table 2

# tidy from broom package changes summary information into data.frames
ols_m1 <- tidy(summary(lm_model))
# select only the important columns

# Change p values into vectors for each model
p <- p_val(ols_m1$p.value)

# Round statistic values to 2 digits and change to vector as well, keep zeros
t <- sprintf("%.2f", ols_m1$estimate)

# Join statistic values and p asterisks
ols_m1$estimate <- paste(t, p)

# Find R squared % for each model
r <- summary(lm_model)$r.squared * 100

# Round them to 2 digits
r <- round(r, 2)
# rename row names of table
rownames(ols_m1) <- c("(Intercept)", "lsdPC", "crowdedPC", "noQualPC")

ols_m1 <- ols_m1[1:3]
```


```{r, global2, echo=F, results=T}
# tell Kable to not show anything for NA values
options(knitr.kable.NA = "")
# Table 2
kable(ols_m1,
  digits = 2, caption = "Multiple Regression Model †",
  linesep = "",
  col.names = c(
    "Variable",
    "Beta-coefficient",
    "Standard Errors"
  )
) %>%
  # footnote for significance
  footnote(
    general_title = "",
    general = c(
      "† R2 = 40.4",
      "*** Significant at the 0.001 level"
    )
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "float_left"
  )
```

```{r, bw, echo=F, eval=T}
# has to use sp for polys
spLSOA <- nwLSOA %>%
  as_Spatial()

# find optimal bandwidth using cross validation
bwB <- bw.gwr(unemployPC ~ lsdPC + crowdedPC + noQualPC, data = spLSOA, kernel = "bisquare")

save(bwB, file = "./data/bwB.RData")
```

```{r, echo=F}
load(file = "./data/bwB.RData")
```

```{r, gwrB, eval=T, echo=F}
# has to use sp for polys
spLSOA <- nwLSOA %>%
  as_Spatial()

gwrB <- gwr.basic(unemployPC ~ lsdPC + crowdedPC + noQualPC, data = spLSOA, bw = bwB, kernel = "bisquare")

save(gwrB, file = "./data/gwrB.RData")
```

```{r, echo=F}
load(file = "./data/gwrB.RData")
```

```{r, mc, eval=F}
# has to use sp for polys
spLSOA <- nwLSOA %>%
  as_Spatial()

gwr_mc <- gwr.montecarlo(unemployPC ~ lsdPC + crowdedPC + noQualPC, data = spLSOA, bw = bwB, kernel = "bisquare")
save(gwr_mc, file = "./data/gwr_mc.RData")
```

```{r, eval=T}
load(file = "./data/gwr_mc.RData")
p <- gwr_mc
```

```{r, gwr, results=T}
gwr_model <- gwrB$lm
## Prepare data for Table 2

# tidy from broom package changes summary information into data.frames
gwr_m1 <- tidy(summary(gwr_model))
# select only the important columns

# Change p values into vectors for each model
p <- p_val(p)

# Round statistic values to 2 digits and change to vector as well, keep zeros
t <- sprintf("%.2f", gwr_m1$estimate)

# Join statistic values and p asterisks
gwr_m1$estimate <- paste(t, p)

# Find R squared % for each model
r <- summary(gwr_model)$r.squared * 100

# Round them to 2 digits
r <- round(r, 2)
# rename row names of table
rownames(gwr_m1) <- c("unemployPC (Intercept)", "lsdPC", "crowdedPC", "noQualPC")

gwr_m1 <- gwr_m1[1:3]

# tell Kable to not show anything for NA values
options(knitr.kable.NA = "")
# Table 2
kable(gwr_m1,
  digits = 2, caption = "Geographically Weighted Regression Model; p values determined by Monte Carlo Random Simulation †",
  linesep = "",
  col.names = c(
    "Variable",
    "Estimate",
    "Standard Errors"
  )
) %>%
  # footnote for significance
  footnote(
    general_title = "",
    general = c(
      "† R2 = 40.4",
      "*** Significant at the 0.001 level"
    )
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    position = "float_right"
  )
```

## GWR Model

Once the bandwidth was determined, the model could be fit using the function `gwr.basic` from the `GWmodel` package. Table \@ref(tab:gwr) indicates that the coefficients of all variables chosen in this study vary significantly over space $(p < 0.001)$. This variation can be mapped (see Figure \@ref(fig:gwr_maps)). What stands on this collection of maps, is that despite a global positive correlation between the rate of unemployment and all other chosen predictor variables, as would be expected, there are some areas for which there is a positive coefficient. Despite a negative global correlation, the coefficient of no crowded against unemployment is shown as positive in many LSOAs, despite high negative coefficients in a small number of isolated LSOAs \@ref(fig:gwrmaps), with the same true for no qualifications.

Monte Carlo techniques [@Hope1968] allow for testing the hypothesis that variables $(\beta_{ij})$ are not significantly affected by their spatial location, i.e. if randomly assigned to spatial locations, are the results given, as the GWR is calibrated significantly affected [@Brunsdon1998], provided by the function `gw.montecarlo`.

Table \@ref(tab:gwr) gives the result of this analysis, all variables appear to correlate with their spatial estimate $(\sqrt{v_j})$ significantly, and at a level far above the standard error relate to standard error$(SE(\beta_j))$. In addition, the Local R2 values \@ref(fig:gwrmaps) indicate that there is variation in how reliable the estimates are, with lower values indicating lower reliability, overall, with the monte carlo analysis indicates a good overall fit for the GWR model.

```{r, gwrmaps, fig.cap="Variation in the R Squared values (top - left). Coefficients of Illness (top - right). Coefficients of Crowded (bottom - left). Coefficients of No Quals (bottom - right)"}

gwSF <- gwrB$SDF %>%
  st_as_sf() %>%
  st_transform(4326)

map_func <- function(x) {
  bg + tm_shape(gwSF) +
    tm_fill(
      col = x,
      midpoint = 0,
      n = 5, # number of bins
      style = "pretty", # fisher jenks bins
      palette = "RdYlGn"
    ) +
    tm_layout(
      legend.format = list(digits = 1),
      frame = F,
      asp = .7 / 1
    )
}

gwList <- colnames(gwSF[c(2:4, 18)])
gwList <- gwList[c(1:4)] # remove geometry


maps <- lapply(gwList, map_func)

tmap_arrange(maps, ncol = 2, nrow = 2)
```

# Conclusion

It appears clear that while global regression and correlation may indicate unexpected results when comparing unemployment with geodemographic variables, utilising GWR can give insight into how much variation there is between coefficients locally, without obscuring the results by considering the study area as one unit.

# References {-}

<div id="refs"></div>

# Appendix I: Preprocessing {-}

### Liverpool Logo

```{r ref.label='logo', echo = T, eval = F}

```

### Default Chunk Settings

```{r ref.label='setup', echo = T, eval = F}

```

### Libraries

```{r ref.label='libs', echo = T, eval = F}

```

### Employment Data

```{r ref.label='employ', echo = T, eval = F}

```

### No Qualifications

```{r ref.label='qual', echo = T, eval = F}

```

### Crowded

```{r ref.label='rpp', echo = T, eval = F}

```

### Join all data to LSOAs

```{r ref.label='join', echo = T, eval = F}

```

### Contiguous Weighting

```{r ref.label='cont_weight', echo = T, eval = F}

```

### Inverse Weighting

```{r ref.label='weights', echo = T, eval = F}

```

### P Value Function

```{r ref.label='pval', echo = T, eval = F}

```

### Local Moran's I for each Weighting Scheme

```{r ref.label='weighting', echo = T, eval = F}

```

### Find Optimal Bandwidth

```{r ref.label='bw', echo = T, eval = F}

```

### Monte Carlo Randomisation

```{r ref.label='mc', echo = T, eval = F}

```
