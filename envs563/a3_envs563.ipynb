{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Targeting areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "# allows plot outputs to appear inline\n",
    "%matplotlib inline\n",
    "\n",
    "## Imports\n",
    "# statistical data visualisation\n",
    "import seaborn as sns\n",
    "# general data analysis\n",
    "import pandas as pd\n",
    "# spatial data analysis\n",
    "import pysal as ps\n",
    "# spatial data manipulation and visualisation\n",
    "import geopandas as gpd\n",
    "# mathematical functions and arrays\n",
    "import numpy as np\n",
    "# general visualisation utilities\n",
    "import matplotlib.pyplot as plt\n",
    "# use operating system dependent functions, e.g. working directory\n",
    "import os     \n",
    "# regular Expressions, search for a pattern in strings\n",
    "import re\n",
    "# find all pathnames in a directory that match a specific extension\n",
    "import glob\n",
    "# add text and shape effects such as shadows\n",
    "import matplotlib.patheffects as path_effects\n",
    "# retrieve and write to disk tile maps from the internet into geospatial raster files, used for basemaps\n",
    "import contextily as ctx\n",
    "# allows point geometry to be added to coordinates\n",
    "from shapely.geometry import Point\n",
    "# read html data and convert into just text\n",
    "from bs4 import BeautifulSoup\n",
    "# used for DBSCAN clustering\n",
    "from sklearn.cluster import dbscan\n",
    "# used for KMeans clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# allows inset figures\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "# able to change values from one crs to another\n",
    "from pyproj import Proj, transform\n",
    "# find the distance between two inputs\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# get rid of warnings (only for submission)\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The focus of this analysis is to determine the potential for infrastructural\n",
    "improvements in Liverpool that may lead to regeneration, and with that the\n",
    "potential for reduced crime (Thomson, Atkinson, Petticrew, & Kearns, 2006)⁠⁠, in\n",
    "areas that are deemed to be crime hotspots, and have shown the potential for an\n",
    "increase in popularity.\n",
    "\n",
    "## Data\n",
    "\n",
    "### Liverpool LSOA Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to LSOA shapefile\n",
    "lsoas_path = './data/liverpool/shapefiles/Liverpool_lsoa11.shp'\n",
    "# Read polygons into a geopandas.geodataframe, set the index to LSOA codes\n",
    "lsoas = gpd.read_file(lsoas_path).set_index('LSOA11CD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to names points (Liverpool City centre, Edge Hill, etc.)\n",
    "names_path = './data/names/NamedPlace.shp'\n",
    "# Read into geopandas.geodataframe\n",
    "names = gpd.read_file(names_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Select only the Liverpool City Centre point\n",
    "cityCentre = names.loc[names['distname'] == 'LIVERPOOL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liverpool Postcodes Coordinate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coordinate data for all Liverpool Postcodes into geopandas point geodataframe ##\n",
    "# Read North West postcode data (huge file)\n",
    "NWPC = pd.read_csv('./data/NWPC.csv', index_col = 'Postcode')\n",
    "\n",
    "# Combine Easting and Northing to one column called geometry\n",
    "NWPC['geometry'] = list(zip(NWPC.Easting, NWPC.Northing))\n",
    "# Add point geometry\n",
    "NWPC['geometry'] = NWPC['geometry'].apply(Point)\n",
    "# Set crs to british national grid\n",
    "NWPC.crs = {'init': 'epsg:27700'}\n",
    "# Turn into a geopandas.geodataframe\n",
    "NWPC = gpd.GeoDataFrame(NWPC, crs=NWPC.crs, geometry=NWPC['geometry'])\n",
    "\n",
    "# Select only LSOAs from the liverpool local authority district code\n",
    "# and rename to lpc (Liverpool Postcodes)\n",
    "lpc = NWPC.loc[NWPC['District Code'] == 'E08000012']\n",
    "# Remove useless columns\n",
    "lpc = NWPC.iloc[:,[3,4,9]]\n",
    "# (Re)Append the geometry\n",
    "lpc['geometry'] = NWPC[['geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent Planning Descisions (past 60 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web pages downloaded as python couldn't strip the data, 66 in total\n",
    "## Was able to use urllib but couldn't figure out how to generate a search query through it :(\n",
    "\n",
    "s = \"\" # define s as an empty string for later\n",
    "\n",
    "# Define path and file extension\n",
    "path = './data/planninghtml/*.html'   \n",
    "# Glob lists all files with .html extension in path\n",
    "files = glob.glob(path)\n",
    "\n",
    "# For loop to read each html file in the directory\n",
    "for name in files:\n",
    "    # Define name when opened as f, 'with' closes file when read (avoids problems with files left open)\n",
    "    with open(name) as f:\n",
    "        # Beautiful soup can pull the html data to convert to text\n",
    "        soup = BeautifulSoup(f)\n",
    "        # Convert into one large string from each .html\n",
    "        s += soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Planning Postcodes dataframe ##\n",
    "## Regex from https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom#Validation\n",
    "## Finds all valid postcodes contained within string 's' (regular expression)\n",
    "ppc = re.findall(r'[A-Z]{1,2}[0-9R][0-9A-Z]? [0-9][A-Z]{2}', s)\n",
    "\n",
    "# Convert from string into pandas dataframe\n",
    "ppc = pd.DataFrame(ppc)\n",
    "# Rename first column to postcode\n",
    "ppc.columns = ['postcode']\n",
    "# Set postcode column as index\n",
    "ppc = ppc.set_index('postcode')\n",
    "# Add a Planning Application counts column for combining all Liverpool postcodes later\n",
    "ppc['planning'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join planning postcodes to all Liverpool postcodes\n",
    "ppc = lpc.join(ppc)\n",
    "# Drop NA values (postcodes with no recent planning decisions)\n",
    "ppc = ppc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find planning application counts per LSOA ##\n",
    "# Right spatial join Postcodes with planning counts to Liverpool LSOAs\n",
    "# Right join uses keys from ppc, retains only LSOA geometry\n",
    "join = gpd.sjoin(ppc, lsoas, how='right')\n",
    "# Remove columns not needed\n",
    "join = join.drop(['Easting', 'Northing', 'District Code'], axis=1)\n",
    "# Rename index to LSOA11CD to merge with LSOA geometry later\n",
    "join.index.names = ['LSOA11CD']\n",
    "# Groupby combines all duplicate LSOA and sums postcode counts, rename df to planning\n",
    "planning = join.groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to house price shapefile\n",
    "hp = './data/houseprice/shapefiles/E08000012_all.shp'\n",
    "# Read into geopandas and set index as LSOAs\n",
    "hp = gpd.read_file(hp).set_index('LSOA11C')\n",
    "# Rename index to be consistent\n",
    "hp.index.names = ['LSOA11CD']\n",
    "# Change NAs to 0 (NA is LSOA with no houses sold)\n",
    "hp = hp.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find total number of properties sold 2005 - 2015\n",
    "Sales1995_2005 = hp['fr_2005'] + hp['fr_2006'] + hp['fr_2007'] + hp['fr_2009'] + hp['fr_2010'] + \\\n",
    "                      hp['fr_2011'] + hp['fr_2012'] + hp['fr_2013']+ hp['fr_2015'] + hp['fr_2015']\n",
    "\n",
    "# Find total number of properties sold 1995 - 2005\n",
    "Sales2005_2015 =   hp['fr_1995'] + hp['fr_1996'] + hp['fr_1997'] + hp['fr_1999'] + hp['fr_2000'] + \\\n",
    "                      hp['fr_2001'] + hp['fr_2002'] + hp['fr_2003']+ hp['fr_2005'] + hp['fr_2005'] \n",
    "\n",
    "# Find the relative percentage change in sales between the two decades\n",
    "hp['Sales_Change'] = (Sales2005_2015 / Sales1995_2005)*100\n",
    "\n",
    "# Rename the dataframe to houseSales, remove all other columns\n",
    "houseSales = hp[['Sales_Change']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Various data in regards to employment and travel\n",
    "## Travel time, Destination and Origin indicators of employment locations\n",
    "\n",
    "# Define path to csv\n",
    "emp_path = './data/employment.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the employment transport csv and set index as LSOA\n",
    "emp = pd.read_csv(emp_path, index_col='EMPLO001')\n",
    "# Select only LSOA from the liverpool local authority district\n",
    "emp = emp.loc[emp.EMPLO003 == 'E08000012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Travel time, destination and origin indicators to key sites and services\n",
    "# by Lower Super Output Area (LSOA) (ACS05)\n",
    "###\n",
    "\n",
    "# Select: Travel time to nearest employment centre by Public Transport/walk\n",
    "pTransport = emp[['EMPLO008']]\n",
    "\n",
    "# Rename EMPLO104 to dist_pt (distance by public transport)\n",
    "pTransport = pTransport.rename(columns={'EMPLO008': 'dist_pt'})\n",
    "# Rename index to match lsoa naming\n",
    "pTransport.index.names = ['LSOA11CD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadband Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to broadband speed point shapefile\n",
    "bb_path = './data/broadband/shapefiles/E08000012.shp'\n",
    "# Read into a geopandas.geodataframe\n",
    "bb = gpd.read_file(bb_path)\n",
    "# Combine <2mb column and <10mb column to make an overall slow broadband column\n",
    "bb['<10mb'] = bb['num_sl'] + bb['num_av']\n",
    "# Set index to postcodes\n",
    "bb = bb.set_index('pcd')\n",
    "# Set crs as British National Grid\n",
    "bb.crs = {'init': 'epsg:27700'}\n",
    "# Turn into a geopandas.geodataframe again\n",
    "bb = gpd.GeoDataFrame(bb, crs=bb.crs, geometry=bb['geometry'])\n",
    "# Rename index to postcode\n",
    "bb.index.names = ['postcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join uses keys from bb, retains only LSOA geometry\n",
    "broadband = gpd.sjoin(bb, lsoas, how='right')\n",
    "# Average percentage of homes in each postcode within LSOA that have less than 10mb download speed\n",
    "broadband = broadband.groupby(level=0).mean()\n",
    "# Rename index for consistency\n",
    "broadband.index.names = ['LSOA11CD']\n",
    "# Define new dataframe just with slow broadband\n",
    "slowBroadband = broadband[['<10mb']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path and file extension\n",
    "path = './data/crime/*'   \n",
    "# Glob lists all files with .csv extension in path, data was provided in individual files\n",
    "# recursive=True allows glob to search each file\n",
    "files = glob.glob('./data/crime/**/*.csv', recursive=True)\n",
    "# Combines all csv in 'files' into one large dataframe (Concatenate)\n",
    "crime = pd.concat([pd.read_csv(f) for f in files])\n",
    "\n",
    "# Find non numeric values in dataframe\n",
    "find = crime.applymap(np.isreal)\n",
    "# Use Longitude to check there are no headers inside csv (none found, maybe pandas removes them)\n",
    "find = find.loc[find['Longitude'] == False]\n",
    "\n",
    "# Subset randomly to 1/10 the dataframe, turns out 180k rows was too ambitious later on\n",
    "crime = crime.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index as LSOA\n",
    "crime = crime.set_index('LSOA code')\n",
    "# Remove unecessary columns\n",
    "crime = crime[['Crime type', 'Longitude', 'Latitude']]\n",
    "# Rename index for consistency\n",
    "crime.index.names = ['LSOA11CD']\n",
    "\n",
    "# Combine Easting and Northing to one column called geometry\n",
    "crime['geometry'] = list(zip(crime['Longitude'], crime['Latitude']))\n",
    "# Add point geometry\n",
    "crime['geometry'] = crime['geometry'].apply(Point)\n",
    "# Set crs to WGS84 (this is the crs the data comes as)\n",
    "crime.crs = {'init': 'epsg:4326'}\n",
    "# Turn into a geopandas geodataframe\n",
    "crime = gpd.GeoDataFrame(crime, crs=crime.crs, geometry=crime['geometry'])\n",
    "# Reproject crs as British National Grid\n",
    "crime = crime.to_crs({'init': 'epsg:27700'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For loop to change the Lat Lon to Easting and Northing for British National Grid ##\n",
    "\n",
    "# Allows the for loop to numerically iterate through index\n",
    "crime = pd.DataFrame.reset_index(crime)\n",
    "\n",
    "# Define current crs values and required crs values\n",
    "inProj = Proj(init='epsg:4326')\n",
    "outProj = Proj(init='epsg:27700')\n",
    "\n",
    "# Loop for each row to change values, iterrows() required so that index can be used for iteration\n",
    "for i, row in crime.iterrows():\n",
    "    # Easting and Northing output for each row, transform from pyproj takes input proj and output proj\n",
    "    # have to specify the exact location of values so use df.at to do this\n",
    "    e, n = transform(inProj,outProj, crime.at[i, 'Longitude'], crime.at[i, 'Latitude'])\n",
    "    \n",
    "    # Input each new value, again need to use df.at\n",
    "    crime.at[i, 'Longitude'] = e\n",
    "    crime.at[i, 'Latitude'] = n\n",
    "    \n",
    "# this loop took too long with 180k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename from Lat Lon to Easting and Northing\n",
    "crime.rename(columns={'Longitude':'Easting', 'Latitude': 'Northing'}, inplace=True)\n",
    "# Lots of points fall outside the lsoa polygons, inner join to find those that fall inside\n",
    "crime = gpd.sjoin(crime, lsoas, how='inner')\n",
    "# Remove column created in the join\n",
    "crime = crime.drop(['index_right'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "### Global Spatial Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join all polygon attributes into one dataframe\n",
    "l = [slowBroadband, houseSales]\n",
    "dfs = planning.join(pTransport)\n",
    "# Rename df to spatial lag for analysis\n",
    "spatialLag = dfs.join(l)\n",
    "\n",
    "# Add the LSOA geometry by shared column (LSOA names)\n",
    "spatialLag = spatialLag.merge(lsoas, on='LSOA11CD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find queen weights for the LSOAs\n",
    "w_queen = ps.weights.Queen.from_dataframe(spatialLag)\n",
    "# Row standardise the queen weights (sum of rows add to 1)\n",
    "w_queen.transform = 'R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can't seem to iterate Moran's I so did it manually\n",
    "# Call the Moran's I function for each attribute\n",
    "mi1 = ps.Moran(spatialLag['planning'], w_queen)\n",
    "mi2 = ps.Moran(spatialLag['dist_pt'], w_queen)\n",
    "mi3 = ps.Moran(spatialLag['<10mb'], w_queen)\n",
    "mi4 = ps.Moran(spatialLag['Sales_Change'], w_queen)\n",
    "\n",
    "# Create a dataframe to display all observed values\n",
    "MI = pd.DataFrame({'Planning': [mi1.I, mi1.p_sim],\n",
    "                   'Distance by Public Transport': [mi2.I, mi2.p_sim],\n",
    "                   'Slow Broadband': [mi3.I, mi3.p_sim],\n",
    "                   'House Sales Change': [mi4.I, mi4.p_sim]},\n",
    "                   index=['Morans I', 'P Values'])\n",
    "# Round to 3 decimals\n",
    "MI = np.round(MI, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1: Morans I for each attribute with associated P values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI.T # Show MI values and p values as a table (transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spatial lag of each attribute was determined, and through Moran’s I used to\n",
    "assess the level of global spatial autocorrelation for each attribute.\n",
    "\n",
    "All show\n",
    "a slight significant positive spatial autocorrelation (Table 1) , however public\n",
    "transport data appears to give NA, likely due to the data not being strictly\n",
    "continuous. These results suggest infrastructual improvement to the broadband in\n",
    "an area may improve neighbouring LSOAs, for example an improvement to an\n",
    "exchange would improve broadband in all houses connected to that exchange.\n",
    "\n",
    "### Geodemographic Analysis\n",
    "\n",
    "Employment and transport data was missing coverage for several LSOAs. A function\n",
    "was created to allow for missing data imputation, which takes the attribute\n",
    "means of K Means clusters when NA columns are excluded, and recalculates new\n",
    "clusters using these values. This takes a common missing data handling technique\n",
    "(Acock, 2005)⁠⁠, and allows for a more accurate data imputation through K Means,\n",
    "which normally cannot handle NA values (Mesquita, Gomes, & Rodrigues, 2016)⁠.\n",
    "Data was also standardised before analysis to ensure attributes are directly\n",
    "comparable, and to remove the potential for bias weighting during K Means\n",
    "clustering (Mohamad & Usman, 2013)⁠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to standardise all rows within a dataframe ##\n",
    "## Couldn't get it to ignore geometry properly with 'continue' so only use on regular dataframes ##\n",
    "\n",
    "def standardise(df):\n",
    "    \"\"\"\n",
    "    Standardise every numeric column in a dataframe.\n",
    "    All columns must be numeric with no geometry.\n",
    "    ...\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    df    : pandas.DataFrame\n",
    "            Dataframe to be standardised\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    df    : pandas.DataFrame\n",
    "            Standardised dataframe\n",
    "    \"\"\"\n",
    "    # Loop to iterate through each column\n",
    "    for column in df:\n",
    "        # Standardise each column by taking away the mean and dividing by standard dev\n",
    "        df[column] = df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "    # Return the dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KMeans with Null function ##\n",
    "## Allows for all columns to be used including columns containing nulls ##\n",
    "## Works best with many non null columns and when all rows required for analysis ##\n",
    "\n",
    "def kmeans_withnull(df, num):\n",
    "    \"\"\"\n",
    "    Allows K Means function with null values.\n",
    "    Substitutes means from non null K Means clusters into dataframe to run.\n",
    "    Rows in dataframe must all be numeric.\n",
    "    Created with the ideal that all rows must be maintained, and there are multiple columns with no NA values.\n",
    "    Must have at least one column with no NA, advised that majority of columns have no NA.\n",
    "    ...\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    df    : pandas.DataFrame\n",
    "            Dataframe to run KMeans_withnull analysis\n",
    "    \n",
    "    num   : integer\n",
    "            Number of KMeans clusters\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    df    : dataframe\n",
    "            Input dataframe with KMeans cluster labels appended as column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Take num as the imput for number of clusters\n",
    "    kmeans = KMeans(n_clusters=num)\n",
    "    # Save the index of input dataframe for later\n",
    "    index = df.index.values\n",
    "    \n",
    "    # Drop columns with any NA values (Keep all rows)\n",
    "    dropCol = df.dropna(axis='columns')\n",
    "    # Run KMeans on these columns\n",
    "    k = kmeans.fit(dropCol)\n",
    "    # Add column of KMeans clusters to original dataframe\n",
    "    df['kMeans'] = k.labels_\n",
    "    \n",
    "    # For loop to find mean values for each cluster per column\n",
    "    for i in range(num):\n",
    "        # Easier indexing\n",
    "        c = df.set_index('kMeans')\n",
    "        # Group by kMeans and find the mean values of each column\n",
    "        c = c.groupby('kMeans').mean()\n",
    "\n",
    "        # For each kMeans cluster, add average values from above for all NA values\n",
    "        # This allows to estimate likely missing values based on their cluster when excluded\n",
    "        df.loc[df['kMeans'] == i] = df.loc[df['kMeans'] == i].fillna(c.loc[i])\n",
    "    \n",
    "    # Reset the index back to normal\n",
    "    df.set_index(index)\n",
    "    # Remove old kMeans clusters\n",
    "    df = df.drop(['kMeans'], axis=1)\n",
    "    \n",
    "    # Re-calculate KMeans using new imputed mean values\n",
    "    k = kmeans.fit(df)\n",
    "    # Add the new KMeans clusters\n",
    "    df['kMeans'] = k.labels_\n",
    "    \n",
    "    # Return dataframe with KMeans clusters\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join all attributes into one dataframe\n",
    "infra = [slowBroadband, pTransport]\n",
    "KM = planning.join(houseSales)\n",
    "KM = KM.join(infra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise each row with my function defined above\n",
    "KM_z = standardise(KM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *KMeans Elbow to choose K clusters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to drop NAs for this method\n",
    "KM_NA = KM_z.dropna()\n",
    "\n",
    "## KMeans determine k (optimal number of clusters) code adapted from:\n",
    "# https://pythonprogramminglanguage.com/kmeans-elbow-method/\n",
    "distortions = [] # empty list\n",
    "K = range(1,10) # define range of K (number of clusters)\n",
    "# iterate through number of clusters\n",
    "for k in K:\n",
    "    # choose k clusters for KMeans\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    # fit the model\n",
    "    kmeanModel.fit(KM_NA)\n",
    "    # Find the level of distortion using total distance from cluster central points (cdist)\n",
    "    distortions.append(sum(np.min(cdist(KM_NA, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / KM_NA.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "## Do not want to display in final document\n",
    "\n",
    "##plt.plot(K, distortions, 'bx-')\n",
    "##plt.xlabel('k')\n",
    "##plt.ylabel('Distortion')\n",
    "##plt.title('The Elbow Method showing the optimal k')\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *K Means Analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # Make sure my K Means clusters don't change\n",
    "\n",
    "# Use my function above to run K Means with null values to produce 3 clusters\n",
    "KM_z = kmeans_withnull(KM_z, 3)\n",
    "# Add the LSOA geometry\n",
    "KM_z = KM_z.merge(lsoas, on='LSOA11CD')\n",
    "# Set the crs as British National Grid\n",
    "KM_z.crs = {'init': 'epsg:27700'}\n",
    "# Convert to a GeoDataFrame\n",
    "KM_z = gpd.GeoDataFrame(KM_z, crs=KM_z.crs, geometry=KM_z.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1] # I use an array here in case I want more than 1 cluster\n",
    "\n",
    "# Define the K Means sub group containing just the cluster of interest\n",
    "KMS_z = KM_z.loc[KM_z['kMeans'].isin(a)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a hacky border for the chosen cluster\n",
    "# Change name to avoid altering data\n",
    "selected_cluster = KMS_z\n",
    "# Dissolve all polygons into one\n",
    "selected_cluster = selected_cluster.dissolve(by='kMeans')\n",
    "# Change back into a geodataframe\n",
    "selected_cluster = gpd.GeoDataFrame(selected_cluster, crs=lsoas.crs, geometry=selected_cluster.geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Plot K Means*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ax to Spherical Mercator, required to use the basemap, uses LSOA polys to choose raster image/location\n",
    "# lsoas not needed to alpha set to 0\n",
    "ax_bm = lsoas.to_crs(epsg=3857).plot(figsize=(7.5, 9), alpha=0)\n",
    "# Add the default basemap from Contextily\n",
    "ctx.add_basemap(ax_bm, alpha=0.5)\n",
    "\n",
    "# Plot K Means choropleth showing the three clusters, include legend, and custom colour scale\n",
    "# have to change all crs to project onto basemap\n",
    "KM = KM_z.to_crs(epsg=3857).plot(ax=ax_bm, column='kMeans', categorical=True, alpha=1, linewidth=.5, legend=True,\n",
    "          edgecolor='black', cmap='tab20c')\n",
    "\n",
    "# Add red border to LSOAs from chosen cluster, patheffects used for a shadow effect\n",
    "selected_cluster.to_crs(epsg=3857).plot(ax=ax_bm, linewidth=1, edgecolor='red', facecolor='none',\n",
    "                                        path_effects=[path_effects.Stroke(linewidth=3, foreground='black',\n",
    "                                        alpha=0.4), path_effects.Normal()])\n",
    "\n",
    "# Change crs of City Centre point to spherical mercator\n",
    "cityCentre_bm = cityCentre.to_crs(epsg=3857)\n",
    "# Take x and y of City Centre from geometry\n",
    "x = cityCentre_bm.geometry.x\n",
    "y = cityCentre_bm.geometry.y\n",
    "\n",
    "# Add label for the city centre\n",
    "text = ax_bm.annotate('City Centre', color='black', xy=(x,y), alpha=1)\n",
    "# Create a 'buffer' effect for visibility\n",
    "text.set_path_effects([path_effects.Stroke(linewidth=3, foreground='white', alpha=0.8),\n",
    "                       path_effects.Normal()])\n",
    "\n",
    "# Remove axis ticks and labels\n",
    "ax_bm.set_yticklabels([])\n",
    "ax_bm.set_xticklabels([])\n",
    "ax_bm.set_xticks([])\n",
    "ax_bm.set_yticks([])\n",
    "\n",
    "# Add a title\n",
    "plt.title('Figure 1: Geodemographic Classification of Liverpool by Infrastructure and Popularity',\n",
    "          fontsize=14, weight='bold')\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean value for each attribute in each cluster\n",
    "K3 = KM_z.groupby('kMeans').mean()\n",
    "# Rename attribute names for clarity\n",
    "K3 = K3.rename(index=str, columns={\"planning\": \"Planning\", \"dist_pt\": \"Distance by Public Transport\",\n",
    "                              \"<10mb\": \"Slow Broadband\", \"Sales_Change\": \"House Sales Change\"})\n",
    "# Round to 3 decimals\n",
    "K3 = np.round(K3, decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2: Average Attribute values by K Means Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K3.T # Use to choose which cluster(s) to keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent public planning decisions by postcode were stripped from the Liverpool\n",
    "City Council website and used as a metric to determine the desirability of areas\n",
    "for regeneration of existing buildings. This, paired with the proportional\n",
    "increase in house sales between 1995-2005, and 2005-2015 was used to assess the\n",
    "upcoming popularity of an area (Brendan, 2010)⁠⁠, and where focused improvements\n",
    "should be targeted.\n",
    "\n",
    "Public transport and Broadband speed were chosen as\n",
    "targeted infrastructures, the average time houses in a particular postcode are\n",
    "able to use public transport and walking to reach their nearest workzone. Public\n",
    "transport is integral to the ideal of what is considered to be a sustainable\n",
    "city, where public transport encourages alternative methods of travel of which\n",
    "there is a minimised environmental impact in comparison to travel purely by car\n",
    "(Banister, Wood, & Watson, 1997)⁠. Research in the UK makes explicit links\n",
    "between poverty and transport disadvantage (Lucas, 2012)⁠⁠. Past studies have\n",
    "put emphasis on the importance of a suitable broadband infrastructure for\n",
    "enconomic growth (Riddlesden & Singleton, 2014)⁠⁠, and the growth of broadband\n",
    "in households and industry in the UK has transformed various services and\n",
    "businesses through the feasibility to work purelly digitally (Caio, 2008)⁠.\n",
    "Figure 1 shows the clusters determined through a geodemographic analysis based\n",
    "on key infrastructures using a K Means analysis of the chosen attributes. Three\n",
    "clusters were chosen through the \"K Means Elbow Method\" which uses diminishing\n",
    "returns in distortion to determine the optimal number of clusters (Kodinariya &\n",
    "Makwana, 2013)⁠.\n",
    "\n",
    "Cluster 1 (highlighted in red) is chosen to represent LSOAs\n",
    "which share similar levels of infrastructural disadvantage, but with higher than\n",
    "average levels of popularity (Table 1).\n",
    "\n",
    "### DBSCAN to determine crime hotspots\n",
    "\n",
    "#### *DBSCAN analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the number of points 0.1% of the total represents\n",
    "minp = np.round(crime.shape[0] * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # Keep same results\n",
    "\n",
    "# DBSCAN - Density-Based Spatial Clustering of Applications with Noise\n",
    "# Finds core samples of high crime density and expands clusters from them, radis of 400m\n",
    "cs, lbls = dbscan(crime[['Easting', 'Northing']], eps=400, min_samples=minp)\n",
    "\n",
    "# Turn labels into a Series\n",
    "lbls = pd.Series(lbls, index=crime.index)\n",
    "\n",
    "# Find core points from DBSCAN\n",
    "cores = crime.iloc[cs, :]\n",
    "# Find all noise from DBSCAN\n",
    "noise = crime.loc[lbls==-1]\n",
    "# Subtract noise points from all points to find not noise using index difference\n",
    "not_noise = crime.loc[crime.index.difference(noise.index)]\n",
    "\n",
    "# Remove columns and set index\n",
    "not_noise = not_noise.set_index('LSOA11CD')\n",
    "not_noise = not_noise[['Crime type', 'geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Plot Setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find crime points within chosen K Means clusters\n",
    "crime_cluster = gpd.sjoin(crime, KMS_z, how='inner')\n",
    "# Remove index right so I can join again below\n",
    "crime_cluster = crime_cluster.drop(['index_right'], axis=1)\n",
    "\n",
    "# Join lsoa polygons to crime_cluster to create DBSCAN Polygons\n",
    "db_polys = gpd.sjoin(lsoas, crime_cluster, how='inner')\n",
    "# Drop index_right again\n",
    "db_polys = db_polys.drop(['index_right'], axis=1)\n",
    "# Remove all duplicate polygons\n",
    "db_polys = db_polys[~db_polys.index.duplicated(keep='first')]\n",
    "# Find non noise points which fall inside the cluster chosen\n",
    "db_polys = gpd.sjoin(db_polys, not_noise, how='inner')\n",
    "# Again remove duplicate polygons\n",
    "db_polys = db_polys[~db_polys.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an overall border for the LSOAs\n",
    "# Call bg to avoid editing lsoas\n",
    "bg = lsoas\n",
    "# Add column to dissolve by all polygons\n",
    "bg['border'] = 1\n",
    "# Dissolve all polygons into one\n",
    "bg = bg.dissolve(by='border')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *DBSCAN Plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set axis to use spherical mercator projection so open source basemap can be used\n",
    "# chooses the location based on the lsoa polygons\n",
    "## NOTE: all plots on this axis must use spherical mercator\n",
    "ax_bm = lsoas.to_crs(epsg=3857).plot(figsize=(7.5, 9), color='white', alpha=0)\n",
    "# Add default basemap from contextily\n",
    "ctx.add_basemap(ax_bm, alpha=0.5)\n",
    "\n",
    "# Plot all chosen K Mean cluster polygons for context (faded)\n",
    "KMS_z.to_crs(epsg=3857).plot(ax=ax_bm, color='lightgrey', linewidth=0.3, edgecolor='black', alpha=1)\n",
    "\n",
    "\n",
    "# Plot the K Mean cluster polygons that have crime non noise points inside\n",
    "db_polys.to_crs(epsg=3857).plot(ax=ax_bm, color='red', linewidth=0.5, edgecolor='black', alpha=0.8)\n",
    "\n",
    "# Plot DBSCAN noise crime points in grey\n",
    "noise.to_crs(epsg=3857).plot(ax=ax_bm, markersize=15, color='grey', alpha=0.1)\n",
    "# Plot DBSCAN not noise points in orange\n",
    "not_noise.to_crs(epsg=3857).plot(ax=ax_bm, markersize=15, color='orange', alpha=0.1)\n",
    "# Plot DBSCAN core points in red\n",
    "cores.to_crs(epsg=3857).plot(ax=ax_bm, markersize=15, color='red', alpha=0.1)\n",
    "\n",
    "\n",
    "# Add a border to the lsoas, includes some shadowing for aesthetics\n",
    "bg.to_crs(epsg=3857).plot(ax=ax_bm, color='none', edgecolor='black', linewidth=1,\n",
    "                          path_effects=[path_effects.Stroke(linewidth=3, foreground='black', alpha=0.4),\n",
    "                           path_effects.Normal()])\n",
    "\n",
    "# Change crs of City Centre point to spherical mercator\n",
    "cityCentre_bm = cityCentre.to_crs(epsg=3857)\n",
    "# Take x and y of City Centre from geometry\n",
    "x = cityCentre_bm.geometry.x\n",
    "y = cityCentre_bm.geometry.y\n",
    "\n",
    "# Add label for the city centre\n",
    "text = ax_bm.annotate('City Centre', color='black', xy=(x,y), fontsize=12, alpha=0.8)\n",
    "# Create a text 'buffer' effect for visibility\n",
    "text.set_path_effects([path_effects.Stroke(linewidth=3, foreground='white'),\n",
    "                       path_effects.Normal()])\n",
    "\n",
    "# Plot then main title\n",
    "plt.title('Figure 2: DBSCAN Clusters of Crime in Liverpool', fontsize=14, weight='bold')\n",
    "\n",
    "## INSET PLOT\n",
    "# Create axes for inset plot\n",
    "axins = inset_axes(ax_bm, width=\"25%\", height=2.0, loc=3)\n",
    "# Add LSOA background to inset to show context\n",
    "inset = lsoas.plot(ax=axins, color='lightgrey', alpha=0.5)\n",
    "# Add KMeans chosen cluster LSOA in grey\n",
    "KMS_z.plot(ax=axins, color='lightgrey', linewidth=0, edgecolor='black', alpha=1)\n",
    "# Show results of DBSCAN LSOA in red\n",
    "db_polys.plot(ax=axins, linewidth=0.3, edgecolor='red', color='red')\n",
    "\n",
    "# Remove inset axis ticks and labels\n",
    "axins.set_yticklabels([])\n",
    "axins.set_xticklabels([])\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "##\n",
    "\n",
    "# Redimensionate X and Y axes to desired bounds\n",
    "ax_bm.set_ylim(7051000, 7069000)\n",
    "ax_bm.set_xlim(-337500, -324000) # dont set to higher than -337500\n",
    "\n",
    "# Remove axis ticks and labels\n",
    "ax_bm.set_yticklabels([])\n",
    "ax_bm.set_xticklabels([])\n",
    "ax_bm.set_xticks([])\n",
    "ax_bm.set_yticks([])\n",
    "\n",
    "# Display the map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters of high crime are identified in Figure 2 with the use of DBSCAN, this\n",
    "technique clusters crime points that are spatially nearby, shown in red and\n",
    "orange, and considers points with few neighbours to be noise (Grey points;\n",
    "Figure 2). LSOA polygons from the chosen K Means cluster that contain one or\n",
    "more clustered points (non noise) are highlighted in red.\n",
    "\n",
    "Urban regeneration\n",
    "has shown to positively impact areas with lower socioeconomic status and health\n",
    "(Thomson et al., 2006)⁠, where typically there are high levels of crime (Steptoe\n",
    "& Feldman, 2001)⁠. Crime is concentrated towards the city centre, however in a\n",
    "tourist city like Liverpool this is due to the tourist population primarily and\n",
    "less due to the underlying population (Calafat et al., 2011; Hughes et al.,\n",
    "2008).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "#### *Select Final 5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 5 polygons with worst infrastructure, longest time to work zone by public transport\n",
    "# and slowest broadband speeds\n",
    "Chosen = db_polys.sort_values(['dist_pt', '<10mb'], ascending=[False, False]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset table to show only columns of interest\n",
    "ChosenTable = Chosen[['planning', 'dist_pt', '<10mb', 'Sales_Change']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Plot Setup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename bb (broadband) for KDE to ensure it isn't overwritten\n",
    "kdebb = bb\n",
    "# Append column for postcodes with over 50% of homes with less than 10mb download\n",
    "kdebb['slow'] = kdebb['<10mb'] > 50\n",
    "# Use boolean values that =True when >50% of homes within a postcode\n",
    "# have less than 10mb download speeds\n",
    "kdebb['slow'] = kdebb['slow'].astype(int)\n",
    "# Subset into just slow postcodes for KDE plot to show black spots\n",
    "kdebb = kdebb.loc[kdebb['slow'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer bg to create a clean edge for KDE\n",
    "bg = bg.buffer(10000)\n",
    "# Change back into a geodataframe\n",
    "bg = gpd.GeoDataFrame(bg, crs=lsoas.crs, geometry=bg.geometry)\n",
    "# Overlay set difference to remove bg that overlaps the lsoas\n",
    "bg = gpd.overlay(bg, lsoas, how='difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append broadband geometry for KDE plot\n",
    "kdebb['X'] = kdebb.geometry.x\n",
    "kdebb['Y'] = kdebb.geometry.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *DBSCAN Plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup figure and main ax\n",
    "f, ax = plt.subplots(1, figsize=(9, 9))\n",
    "\n",
    "# Plot LSOA background for colour/context\n",
    "lsoas.plot(ax=ax, color='lightgrey', alpha=0.5)\n",
    "\n",
    "# Plot LSOAS greyed out that are removed from this analysis\n",
    "db_polys.plot(ax=ax, color='lightgrey', linewidth=0, edgecolor='black', alpha=1)\n",
    "# Plot broadband KDE underlying LSOAs showing black spots in dark red\n",
    "sns.kdeplot(kdebb['X'], kdebb['Y'], ax=ax, n_levels=20, shade=True, cmap='Reds', alpha=0.3,\n",
    "           shade_lowest=False)\n",
    "# Plot choropleth of 5 chosen LSOAs\n",
    "Chosen.plot(ax=ax, column = Chosen['LSOA11CD'], alpha = 0.8, linewidth=1,\n",
    "            legend = True, edgecolor='black')\n",
    "\n",
    "## Move legend so it doesn't overlap polygon\n",
    "leg = ax.get_legend()\n",
    "leg.set_bbox_to_anchor((0., 0.67, 0.3, 0.3))\n",
    "\n",
    "# Make sure KDE doesn't appear outside MSOA boundary, uses overlay difference\n",
    "bg.plot(ax=ax, color='#f6f6f6', edgecolor='black')\n",
    "\n",
    "# Find x y geometry for city centre point to add label\n",
    "x = cityCentre.geometry.x\n",
    "y = cityCentre.geometry.y\n",
    "\n",
    "# Add label for the city centre\n",
    "text = ax.annotate('City Centre', color='black', xy=(x, y))\n",
    "# Create a text 'buffer' effect for visibility\n",
    "text.set_path_effects([path_effects.Stroke(linewidth=3, foreground='white'),\n",
    "                       path_effects.Normal()])\n",
    "\n",
    "# Set main plot title\n",
    "plt.title('Figure 3: Chosen 5 LSOAs showing Broadband \"Black spots\"', fontsize=14, weight='bold')\n",
    "\n",
    "## INSET PLOT\n",
    "# Add an axis for an inset plot, gives context to zoomed plot\n",
    "axins = inset_axes(ax, width=\"25%\", height=2.0, loc=3)\n",
    "# Add LSOA background to inset\n",
    "inset = lsoas.plot(ax=axins, color='lightgrey', alpha=0.5)\n",
    "\n",
    "# Show faded removed LSOAs from DBSCAN\n",
    "db_polys.plot(ax=axins, color='lightgrey', linewidth=0, edgecolor='black', alpha=1)\n",
    "# Show chosen LSOAs on inset\n",
    "Chosen.plot(ax=axins, linewidth=0.3, edgecolor='red', color='red')\n",
    "\n",
    "# Remove inset axis labels and ticks\n",
    "axins.set_yticklabels([])\n",
    "axins.set_xticklabels([])\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "##\n",
    "\n",
    "# Keep axes proportionate\n",
    "plt.axis('equal')\n",
    "\n",
    "# Redimensionate X and Y axes to desired bounds\n",
    "ax.set_ylim(386000, 397000)\n",
    "ax.set_xlim(330000, 339000)\n",
    "\n",
    "# Remove remove axis labels and ticks (not sure why Y and X showing)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Draw map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename attributes for clarity\n",
    "ChosenTable = ChosenTable.rename(index=str, columns={\"planning\": \"Planning\",\n",
    "                                                     \"dist_pt\": \"Distance by Public Transport\",\n",
    "                                                     \"<10mb\": \"Slow Broadband\",\n",
    "                                                     \"Sales_Change\": \"House Sales Change\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 2: Chosen LSOAs showing all standardised attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChosenTable.T # Show table with 5 LSOAS and their standardised varibles (transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 7 LSOAs left from the DBSCAN analysis, the final 5 are chosen based on the\n",
    "highest levels of need for infrastructural improvement. With this, Figure 3\n",
    "displays the areas in which Broadband is the slowest with a Kernal Density\n",
    "Estimate plot, deemed ‘black spots’, with a higher number of properties with\n",
    "slower broadband in darker red. LSOAs not within black spots, and further from\n",
    "the city centre typically are in need of public transport improvement.\n",
    "\n",
    "Table 2\n",
    "gives a list of the chosen LSOAs with their repective standardised attributes,\n",
    "showing the standard deviation in relation to the mean average of all LSOAs.\n",
    "\n",
    "## References\n",
    "\n",
    "Acock, A. C. (2005). Working with missing values. Journal of Marriage and\n",
    "Family. https://doi.org/10.1111/j.1741-3737.2005.00191.x\n",
    "\n",
    "Banister, Wood, &\n",
    "Watson. (1997). Sustainable cities, transport, energy and urban form,\n",
    "Environment and planning,. Final Report for the URBAN 21 Project, 24,\n",
    "pp.125-143.\n",
    "\n",
    "Brendan, N. (2010). Housing Market Renewal in Liverpool: Locating\n",
    "the gentrification debate in history, context and evidence. Housing Studies,\n",
    "25(5), 715–733. https://doi.org/10.1080/02673037.2010.483587\n",
    "\n",
    "Caio, F. (2008).\n",
    "The Next Phase of Broadband UK: Action now for long term competitiveness - Final\n",
    "Report, (September).\n",
    "\n",
    "Calafat, A., Blay, N., Bellis, M., Hughes, K., Kokkevi,\n",
    "A., Mendes, F., … Tripodi, S. (2011). Tourism, nightlife and violence: a cross\n",
    "cultural analysis and prevention recommendations. Retrieved from\n",
    "http://www.irefrea.eu/uploads/PDF/Calafatetal_2010.pdf\n",
    "\n",
    "Hughes, K., Anderson,\n",
    "Z., Morleo, M., & Bellis, M. A. (2008). Alcohol, nightlife and violence: The\n",
    "relative contributions of drinking before and during nights out to negative\n",
    "health and criminal justice outcomes. Addiction.\n",
    "https://doi.org/10.1111/j.1360-0443.2007.02030.x\n",
    "\n",
    "Kodinariya, T. M., & Makwana,\n",
    "P. R. (2013). Review on determining number of Cluster in K-Means Clustering.\n",
    "International Journal of Advance Research in Computer Science and Management\n",
    "Studies, 1(6), 2321–7782. https://doi.org/10.1109/PDP.2013.84\n",
    "\n",
    "Lucas, K. (2012).\n",
    "Transport and social exclusion: Where are we now? Transport Policy, 20, 105–113.\n",
    "https://doi.org/10.1016/j.tranpol.2012.01.013\n",
    "\n",
    "Mesquita, D. P. P., Gomes, J. P.\n",
    "P., & Rodrigues, L. R. (2016). K-means for datasets with missing attributes:\n",
    "Building soft constraints with observed and imputed values. ESANN 2016 - 24th\n",
    "European Symposium on Artificial Neural Networks, (April), 27–29.\n",
    "\n",
    "Mohamad, I.\n",
    "Bin, & Usman, D. (2013). Standardization and its effects on K-means clustering\n",
    "algorithm. Research Journal of Applied Sciences, Engineering and Technology,\n",
    "6(17), 3299–3303. https://doi.org/10.19026/rjaset.6.3638\n",
    "\n",
    "Riddlesden, D., &\n",
    "Singleton, A. D. (2014). Broadband speed equity: A new digital divide? Applied\n",
    "Geography, 52, 25–33. https://doi.org/10.1016/j.apgeog.2014.04.008\n",
    "\n",
    "Steptoe, A.,\n",
    "& Feldman, P. J. (2001). Neighborhood problems as sources of chronic stress:\n",
    "Development of a measure of neighborhood problems, and associations with\n",
    "socioeconomic status and health. Annals of Behavioral Medicine, 23(3), 177–185.\n",
    "https://doi.org/10.1207/S15324796ABM2303_5\n",
    "\n",
    "Thomson, H., Atkinson, R.,\n",
    "Petticrew, M., & Kearns, A. (2006). Do urban regeneration programmes improve\n",
    "public health and reduce health inequalities? A synthesis of the evidence from\n",
    "UK policy and practice (1980-2004). Journal of Epidemiology and Community\n",
    "Health, 60(2), 108–115. https://doi.org/10.1136/jech.2005.038885\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under ODbL.\n",
    "Department for Transport. (2013). Travel time, destination and origin indicators\n",
    "to key sites and services, by Lower Super Output Area (LSOA) (ACS05). Retrieved\n",
    "from https://www.gov.uk/government/statistical-data-sets/acs05-travel-time-\n",
    "destination-and-origin-indicators-to-key-sites-and-services-by-lower-super-\n",
    "output-area-lsoa\n",
    "\n",
    "Home Office. (2018). data.police.uk. Retrieved from\n",
    "https://data.police.uk/\n",
    "\n",
    "Liverpool City Council. (2018). Recent Decisions.\n",
    "Retrieved from\n",
    "http://northgate.liverpool.gov.uk/PlanningExplorer17/RecentDecisionsSearch.aspx\n",
    "Ordnance Survey. (2018). Code-Point Open. Retrieved from\n",
    "https://www.ordnancesurvey.co.uk/business-and-government/products/code-point-\n",
    "open.html\n",
    "\n",
    "Singleton, A., & Hai, N. (2015). CDRC 2011 OAC Geodata Pack -\n",
    "Liverpool (E08000012). Retrieved from\n",
    "https://data.cdrc.ac.uk/dataset/cdrc-2011-oac-geodata-pack-liverpool-e08000012\n",
    "Singleton, A., Pavlis, M., & University of Liverpool. (2014). CDRC 2014 Fixed\n",
    "Broadband Geodata Pack. Retrieved from\n",
    "https://data.cdrc.ac.uk/dataset/cdrc-2014-fixed-broadband-geodata-pack-\n",
    "liverpool-e08000012\n",
    "\n",
    "Singleton, A., Pavlis, M., & University of Liverpool.\n",
    "(2015). CDRC Median House Prices Geodata Pack (1995-2015). Retrieved from\n",
    "https://data.cdrc.ac.uk/dataset/cdrc-median-house-prices-geodata-\n",
    "pack-1995-2015-liverpool-e08000012"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
